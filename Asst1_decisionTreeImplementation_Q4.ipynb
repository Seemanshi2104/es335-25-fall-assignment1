{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc584dc3",
   "metadata": {},
   "source": [
    "Decision tree Implementation  \n",
    "Q4\n",
    "The complete code is available in experiments.py file.  \n",
    "The overall approach of this code is to analyze the runtime complexity of decision trees experimentally. It does this by generating synthetic datasets of different sizes (varying number of samples N and features M), training and testing a decision tree multiple times to get average training and prediction times, and then plotting how these times scale with N and M. This process is repeated for all four types of decision trees (real/discrete inputs Ã— real/discrete outputs), and the results are saved as plots to compare the empirical runtime with theoretical complexity. In this question, we plotted runtime against N while keeping M=5, and against M while keeping N= 20. \n",
    "  \n",
    "\n",
    "The depth of a decision tree is typically \n",
    "ğ‘‚\n",
    "(\n",
    "log\n",
    "â¡\n",
    "ğ‘\n",
    ")\n",
    ", although in the worst case it can reach \n",
    "ğ‘‚\n",
    "(\n",
    "ğ‘\n",
    ")\n",
    ".\n",
    "\n",
    "Training Time: The theoretical complexity for training a decision tree is O(Mâ‹…NlogN), where \n",
    "ğ‘\n",
    " is the number of samples and \n",
    "ğ‘€\n",
    " is the number of features. Our plot results confirm this: training time increases roughly linearly with both \n",
    "ğ‘€ and \n",
    "ğ‘\n",
    ", since the \n",
    "ğ‘\n",
    "log\n",
    "â¡\n",
    "ğ‘\n",
    "curve appears almost linear, aligning with the expected complexity.\n",
    "\n",
    "Prediction (Testing) Time: The complexity for predicting a single sample is proportional to the depth of the tree, typically \n",
    "ğ‘‚\n",
    "(\n",
    "log\n",
    "â¡\n",
    "ğ‘\n",
    ")\n",
    " for balanced trees. In practice, predictions may terminate at shallower leaf nodes, so the observed time can be slightly lower. When predicting over the full dataset of \n",
    "ğ‘\n",
    "samples, the total testing time scales as \n",
    "ğ‘‚\n",
    "(\n",
    "ğ‘\n",
    "log\n",
    "â¡\n",
    "ğ‘\n",
    ")\n",
    ". Our graphs reflect this trend: prediction time generally increases with \n",
    "ğ‘\n",
    "following an \n",
    "ğ‘‚\n",
    "(\n",
    "log\n",
    "â¡\n",
    "ğ‘\n",
    ")\n",
    "pattern per sample, and remains mostly independent of \n",
    "ğ‘€\n",
    ".\n",
    "Some differences in the plot could be due to variations in the computing environment, such as background processes or CPU throttling, can also affect the timing measurements. Nonetheless, the overall shape of the plots supports the theoretical time complexity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ce9486",
   "metadata": {},
   "source": [
    "### Real Input Real Output  \n",
    "\n",
    "![My Image](time_complexity_plots/real_input_real_output%20wrt%20N%20Training.png)\n",
    "![My Image](time_complexity_plots/real_input_real_output%20wrt%20M%20Training.png)\n",
    "![My Image](time_complexity_plots/real_input_real_output%20wrt%20N%20Testing.png)\n",
    "![My Image](time_complexity_plots/real_input_real_output%20wrt%20M%20Testing.png)\n",
    "\n",
    "### Real Input Discrete Output  \n",
    "![My Image](time_complexity_plots/real_input_discrete_output%20wrt%20N%20Training.png)\n",
    "![My Image](time_complexity_plots/real_input_discrete_output%20wrt%20M%20Training.png)\n",
    "![My Image](time_complexity_plots/real_input_discrete_output%20wrt%20N%20Testing.png)\n",
    "![My Image](time_complexity_plots/real_input_discrete_output%20wrt%20M%20Testing.png)\n",
    "\n",
    "### Discrete Input Real Output  \n",
    "![My Image](time_complexity_plots/discrete_input_real_output%20wrt%20N%20Training.png)\n",
    "![My Image](time_complexity_plots/discrete_input_real_output%20wrt%20M%20Training.png)\n",
    "![My Image](time_complexity_plots/discrete_input_real_output%20wrt%20N%20Testing.png)\n",
    "![My Image](time_complexity_plots/discrete_input_real_output%20wrt%20M%20Testing.png)\n",
    "\n",
    "### Discrete Input discrete Output  \n",
    "![My Image](time_complexity_plots/discrete_input_discrete_output%20wrt%20N%20Training.png)\n",
    "![My Image](time_complexity_plots/discrete_input_discrete_output%20wrt%20M%20Training.png)\n",
    "![My Image](time_complexity_plots/discrete_input_discrete_output%20wrt%20N%20Testing.png)\n",
    "![My Image](time_complexity_plots/discrete_input_discrete_output%20wrt%20M%20Testing.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70be851a",
   "metadata": {},
   "source": [
    "We can observe in all the training plots, there is a roughly linear trend with both N and M and in all the testing plots there is a linear trend with respect to N and constant with respect to M."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d589c8d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
